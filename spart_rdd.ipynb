{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('ABD1')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[\"Porto\", 1900], [\"Lisboa\", 4000]], [\"Name\", \"Zip-Code\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD with sparkContext.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([\"Lisboa\", \"Porto\", \"Faro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD with sparkContext.parallelize() from a python List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Lisboa\", \"Porto\", \"Faro\", \"Coimbra\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD based on an imported file (to the filesystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrdd = spark.sparkContext.textFile('/datasets/Technical Resources-20241101/purplecow.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = spark.sparkContext.textFile(\"/home/mcanudo/Documents/NOVA/ABD/databricks/datasets/Technical Resources-20241101/Poems.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count elements in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of partitions of your RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using map() to upcase all characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataUpper = mydata.map(lambda line: line.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataUpper.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered = mydata.filter(lambda line: \"one\" in line)\n",
    "Filtered.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered1 = mydata.map(lambda line: line.upper()).filter(lambda line: line.startswith('I')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mylist = [50,59.2,59,57.2,53.5,53.2,55.4,51.8,53.6,55.4,54.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Myrdd = spark.sparkContext.parallelize(Mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Myrdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Myrdd.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Myrdd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Myrdd_2 = Myrdd.map(lambda x: x*2)\n",
    "Myrdd_2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.parallelize([\"Do you know that\", \"a horse has one stomach\", \"but a cow has four\"])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_upper = rdd.map(lambda x: x.upper())\n",
    "rdd_upper.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[\"Somebody once told me\",\"the world is gonna roll me\",\"I ain't the sharpest tool in the shed\"]\n",
    "rdd = spark.sparkContext.parallelize(A)\n",
    "\n",
    "ex1 = rdd.flatMap(lambda line : line.split())     #The default character of split() is \" \"\n",
    "ex1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2:\n",
    "given B, create an RDD that only contains the names of the clients that start with the letter A:\n",
    "TIP: you need to separate the name from the purchases (e.g., from this list example=[\"Ronaldo, soccer ball\",\"Orlando, bow\"] the resulting RDD should be [\"Orlando, bow\"]\n",
    "\n",
    "B=[\"Matilde, dress\",\"Maria, pants\",\"Andrew, watch\",\"Oleg, sweatpants\",\"Igor, watch\",\"John, snowglobe\",\"Ashley, perfume\"]\n",
    "\n",
    "B=sc.parallelize(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2list = [\"Matilde, dress\",\"Maria, pants\",\"Andrew, watch\",\"Oleg, sweatpants\",\"Igor, watch\",\"John, snowglobe\",\"Ashley, perfume\"]\n",
    "rdd_ex2 = spark.sparkContext.parallelize(ex2list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_ex2_1 = rdd_ex2.filter(lambda x: x.startswith('A')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_ex2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**\n",
    "\n",
    "given C, create an RDD that is a list of pair RDDs of (name, number).\n",
    "e.g., \"Spencer,6\" should become (spencer,6) and be on the list.\n",
    "\n",
    "C=[\"Jacob,1\",\"Thomas,2\",\"Dominic,3\",\"Jason,4\",\"Robert,5\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "c_list = [\"Jacob,1\",\"Thomas,2\",\"Dominic,3\",\"Jason,4\",\"Robert,5\"]\n",
    "\n",
    "rdd_c = spark.sparkContext.parallelize(c_list)\n",
    "rdd_c1 = rdd_c.map(lambda entry: entry.split(\",\")).map(lambda x: (x[0],x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jacob', '1'),\n",
       " ('Thomas', '2'),\n",
       " ('Dominic', '3'),\n",
       " ('Jason', '4'),\n",
       " ('Robert', '5')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_rdd = spark.sparkContext.textFile('/home/mcanudo/Documents/NOVA/ABD/databricks/datasets/Technical Resources-20241101/alice_in_wonderland.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2697"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Alice was beginning to get very tired of sitting by her sister',\n",
       " 'on the bank, and of having nothing to do:  once or twice she had',\n",
       " 'peeped into the book her sister was reading, but it had no',\n",
       " \"pictures or conversations in it, `and what is the use of a book,'\",\n",
       " \"thought Alice `without pictures or conversation?'\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26450"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_rdd.flatMap(lambda x: x.split()).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
